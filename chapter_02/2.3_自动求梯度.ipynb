{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d386e44e",
   "metadata": {},
   "source": [
    "### ：\n",
    "    Tensor是核心类,将其属性 .requires_grad 设置为 True，将开始追踪(track)在其上的所有操作（即利用链式法则进行梯度传播了）\n",
    "    完成计算后，可以调用 .backward()来完成所有梯度计算，梯度将累积到 .grad属性中。\n",
    "    \n",
    "    不想要被继续追踪，可以调用 .detach()将其从追踪记录中分离出来，这样梯度就传不过去了。\n",
    "    \n",
    "    还可以用 with torch.no_grad()将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d90574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09f015",
   "metadata": {},
   "source": [
    "### 创建一个Tensor并设置 requires_grad=True \n",
    "    也可以通过.requires_grad_()来用in-place的方式改变requires_grad属性\n",
    "    每个Tensor都有一个.grad_fn属性,即该Tensor如果通过运算得到的，则 grad_fn 返回一个与这些运算相关的对象，否则是 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68872253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1a77fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn) # 每个Tensor都有一个.grad_fn属性,即该Tensor如果通过运算得到的，则 grad_fn 返回一个与这些运算相关的对象，否则是 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7c982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x000001A503A581F0>\n"
     ]
    }
   ],
   "source": [
    "y = x + 1\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a287d",
   "metadata": {},
   "source": [
    "### 梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee4b76cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f2e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9167887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad) \n",
    "# grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22cd5730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_2 = x.sum()\n",
    "out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f153ed1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "out_2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "764d1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_3 = x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8ec0ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_() # 在反向传播之前需把梯度清零\n",
    "out_3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3b617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
