{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea01f79",
   "metadata": {},
   "source": [
    "### :\n",
    "    和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf02a3",
   "metadata": {},
   "source": [
    "### 分类问题:\n",
    "    通常使用离散的数值来表示类别,如一张图像的标签为1、2和3这3个数值中的一个"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ac3aa",
   "metadata": {},
   "source": [
    "### softmax回归模型：\n",
    "    跟线性回归一样将输入特征与权重做线性叠加，不同的是softmax回归的输出值个数等于标签里的类别数。\n",
    "    softmax回归同线性回归一样，也是一个单层神经网络，即输出层也是一个全连接层\n",
    "![softmax](./img/3.2/softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d721c",
   "metadata": {},
   "source": [
    "### 交叉熵损失函数:\n",
    "    使用 softmax 运算后可以更方便地与离散标签计算误差\n",
    "    其中,可以像线性回归那样使用平方损失函数,然而，想要预测分类结果正确，并不需要预测概率完全等于标签概率\n",
    "    这时，使用交叉熵（cross entropy）可以更好地衡量两个概率分布的差异\n",
    "    y是长度为q的独热编码向量，y^为预测的概率\n",
    "![cross-entropy](./img/3.2/cross-entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cf64d",
   "metadata": {},
   "source": [
    "### 模型预测及评价:\n",
    "    给定任一样本特征，就可以预测每个输出类别的概率,把预测概率最大的类别作为输出类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1af00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
