{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8632bb",
   "metadata": {},
   "source": [
    "### :\n",
    "    暂退法在前向传播过程中，计算每一隐藏层的同时注入噪声，从表面上看是在训练过程中丢弃（drop out）一些神经元。\n",
    "    在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零，\n",
    "        即该层的隐藏单元将有一定概率被丢弃掉，丢弃概率是超参数\n",
    "    \n",
    "    假设丢弃概率为 p ，那么有 p 的概率 h 会被置零，有 1-p 的概率 h 会除以 1-p 做拉伸\n",
    "![dropout](./img/3.9/dropout.png)\n",
    "\n",
    "    总结：\n",
    "        可以通过使用暂退法应对过拟合现象\n",
    "        暂退法只在训练模型( model.train() )时使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b6884",
   "metadata": {},
   "source": [
    "### 实现 Dropout\n",
    "    以drop_prob的概率丢弃X中的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7d345d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d54a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x,drop_prob):\n",
    "    x = x.float()\n",
    "    assert 0 <= drop_prob <=1 # assert: 当条件为 False 时触发\n",
    "    keep_prob = 1 - drop_prob\n",
    "    if drop_prob == 1: # 所有元素都被丢弃\n",
    "        return torch.zeros_like(x,dtype=torch.float)\n",
    "    if drop_prob == 0: # 所有元素都被保留\n",
    "        return x\n",
    "    \n",
    "    mask = (torch.rand(x.size()) < keep_prob).float()\n",
    "    \n",
    "    return mask * x / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "700051b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11, 12, 13, 14, 15]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11., 12., 13., 14., 15.]]),\n",
       " tensor([[ 0.,  2.,  0.,  6.,  0.,  0., 12.,  0.],\n",
       "         [16., 18.,  0.,  0.,  0.,  0., 28., 30.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(16).view(2, 8)\n",
    "x,dropout(x,0),dropout(x,0.5),dropout(x,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18fa997",
   "metadata": {},
   "source": [
    "#### 定义模型参数\n",
    "    使用Fashion-MNIST数据集，定义一个包含两个隐藏层的多层感知机，其中两个隐藏层的输出个数都是256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1951df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs,num_hidden_1,num_hidden_2,num_outputs = 28 * 28,256,256,10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0abf2",
   "metadata": {},
   "source": [
    "#### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc95f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prop_1,drop_prop_2 = 0.2,0.5\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,num_inputs,num_hidden_1,num_hidden_2,num_outputs):\n",
    "        super(Net,self).__init__()\n",
    "        self.linear_1 = torch.nn.Linear(num_inputs,num_hidden_1)\n",
    "        self.linear_2 = torch.nn.Linear(num_hidden_1,num_hidden_2)\n",
    "        self.linear_3 = torch.nn.Linear(num_hidden_2,num_outputs)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self,x,drop_prop_1,drop_prop_2,is_training=True):\n",
    "        x = x.view(-1,num_inputs)\n",
    "        H1 = self.relu(self.linear_1(x))\n",
    "        \n",
    "        # 只有在训练模型时才使用dropout\n",
    "        if is_training == True:\n",
    "            H1 = dropout(H1,drop_prop_1)\n",
    "            \n",
    "        H2 = self.relu(self.linear_2(H1))\n",
    "        if is_training == True:\n",
    "            H2 = dropout(H2,drop_prop_2)\n",
    "        out = self.linear_3(H2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27709948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (linear_3): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(num_inputs,num_hidden_1,num_hidden_2,num_outputs)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120698b7",
   "metadata": {},
   "source": [
    "#### 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f7ec59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs,lr,batch_size = 10,0.5,256\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr)\n",
    "train_data = torchvision.datasets.FashionMNIST('./data/FashionMNIST',train=True,transform=torchvision.transforms.ToTensor())\n",
    "test_data = torchvision.datasets.FashionMNIST('./data/FashionMNIST',train=False,transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_data,batch_size,shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_data,batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec6bde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(test_iter,net,is_training):\n",
    "    test_acc,test_n = 0.0,0\n",
    "    for x,y in test_iter:\n",
    "        y_hat = net(x,None,None,is_training)\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        \n",
    "        test_acc += (y_hat.argmax(dim=1) == y).float().sum().item()\n",
    "        test_n += y.shape[0]\n",
    "    \n",
    "    return test_acc / test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61366266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_iter,test_iter,loss,num_epochs,optimizer,drop_prop_1,drop_prop_2):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_acc,train_ls,train_n = 0.0,0.0,0\n",
    "        for x,y in train_iter:\n",
    "            y_hat = net(x,drop_prop_1,drop_prop_2,True)\n",
    "            loss = loss_fn(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_acc += (y_hat.argmax(dim=1) == y).float().sum().item()\n",
    "            train_ls += loss.item()\n",
    "            train_n += y.shape[0]\n",
    "        \n",
    "        test_acc = evaluate_acc(test_iter,net,is_training=False)\n",
    "        print('epoch: %d train loss: %.4f train acc: %.3f test acc: %.3f'%(epoch+1,train_ls / train_n\n",
    "                                                                           ,train_acc / train_n,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a80e48c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train loss: 0.0034 train acc: 0.673 test acc: 0.754\n",
      "epoch: 2 train loss: 0.0021 train acc: 0.802 test acc: 0.796\n",
      "epoch: 3 train loss: 0.0018 train acc: 0.832 test acc: 0.837\n",
      "epoch: 4 train loss: 0.0017 train acc: 0.843 test acc: 0.801\n",
      "epoch: 5 train loss: 0.0016 train acc: 0.851 test acc: 0.843\n",
      "epoch: 6 train loss: 0.0015 train acc: 0.862 test acc: 0.815\n",
      "epoch: 7 train loss: 0.0014 train acc: 0.865 test acc: 0.844\n",
      "epoch: 8 train loss: 0.0014 train acc: 0.869 test acc: 0.854\n",
      "epoch: 9 train loss: 0.0013 train acc: 0.874 test acc: 0.856\n",
      "epoch: 10 train loss: 0.0013 train acc: 0.876 test acc: 0.870\n"
     ]
    }
   ],
   "source": [
    "train(net,train_iter,test_iter,loss_fn,num_epochs,optimizer,drop_prop_1,drop_prop_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c784d7",
   "metadata": {},
   "source": [
    "### 简洁实现:\n",
    "    在全连接层后添加Dropout层并指定丢弃概率,\n",
    "    在训练模型时，Dropout层将以指定的丢弃概率随机丢弃上一层的输出元素\n",
    "    在测试模型时（即model.eval()后），Dropout层并不发挥作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71d38ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs,num_hidden_1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(drop_prop_1),\n",
    "    torch.nn.Linear(num_hidden_1,num_hidden_2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(drop_prop_2),\n",
    "    torch.nn.Linear(num_hidden_2,num_outputs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "852a5014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.2, inplace=False)\n",
       "  (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "168982bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad683727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(test_iter,net):\n",
    "    test_acc,test_n = 0.0,0\n",
    "    for x,y in test_iter:\n",
    "        x = x.view(-1,num_inputs)\n",
    "        y_hat = net(x)\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        \n",
    "        test_acc += (y_hat.argmax(dim=1) == y).float().sum().item()\n",
    "        test_n += y.shape[0]\n",
    "    \n",
    "    return test_acc / test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a3a38c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_iter,test_iter,loss,num_epochs,optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_acc,train_ls,train_n = 0.0,0.0,0\n",
    "        for x,y in train_iter:\n",
    "            x = x.view(-1,num_inputs)\n",
    "            y_hat = net(x)\n",
    "            loss = loss_fn(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_acc += (y_hat.argmax(dim=1) == y).float().sum().item()\n",
    "            train_ls += loss.item()\n",
    "            train_n += y.shape[0]\n",
    "        \n",
    "        test_acc = evaluate_acc(test_iter,net)\n",
    "        print('epoch: %d train loss: %.4f train acc: %.3f test acc: %.3f'%(epoch+1,train_ls / train_n\n",
    "                                                                           ,train_acc / train_n,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc5a344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train loss: 0.0011 train acc: 0.894 test acc: 0.811\n",
      "epoch: 2 train loss: 0.0011 train acc: 0.897 test acc: 0.849\n",
      "epoch: 3 train loss: 0.0010 train acc: 0.901 test acc: 0.873\n",
      "epoch: 4 train loss: 0.0010 train acc: 0.902 test acc: 0.872\n",
      "epoch: 5 train loss: 0.0010 train acc: 0.905 test acc: 0.882\n",
      "epoch: 6 train loss: 0.0010 train acc: 0.908 test acc: 0.855\n",
      "epoch: 7 train loss: 0.0009 train acc: 0.909 test acc: 0.866\n",
      "epoch: 8 train loss: 0.0009 train acc: 0.913 test acc: 0.875\n",
      "epoch: 9 train loss: 0.0009 train acc: 0.913 test acc: 0.861\n",
      "epoch: 10 train loss: 0.0009 train acc: 0.915 test acc: 0.889\n"
     ]
    }
   ],
   "source": [
    "train(net,train_iter,test_iter,loss_fn,num_epochs,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd334d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
