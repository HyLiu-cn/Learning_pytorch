{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983442e9",
   "metadata": {},
   "source": [
    "### :\n",
    "    以构造含单隐藏层的多层感知机为例，其中第一层输出大小为 256，第二层输出大小为 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04471ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b760e",
   "metadata": {},
   "source": [
    "### 继承 Module 类来构造模型:\n",
    "    Module类是nn模块里提供的一个模型构造类，是所有神经网络模块的基类，可以通过继承它来定义想要的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952ec979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    \n",
    "    # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "    def __init__(self,num_inputs,num_hiddens,num_outputs):\n",
    "        super(MLP,self).__init__()\n",
    "        self.num_input = num_inputs\n",
    "        self.hidden = torch.nn.Linear(num_inputs,num_hiddens) # 隐藏层\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.output = torch.nn.Linear(num_hiddens,num_outputs) # 输出层\n",
    "        \n",
    "     # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,self.num_input)\n",
    "        h = self.hidden(x)\n",
    "        h = self.act(h)\n",
    "        out = self.output(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbde83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784])\n",
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0642,  0.2110, -0.1054,  0.1317, -0.0462,  0.3714, -0.1830, -0.2279,\n",
       "          0.4248, -0.0802],\n",
       "        [ 0.3755, -0.0987, -0.2511, -0.0330, -0.0279,  0.1122, -0.1949,  0.0027,\n",
       "          0.4253,  0.3917]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_inputs,num_hiddens,num_outputs = 28 * 28,256,10\n",
    "x = torch.randn(2,num_inputs)\n",
    "print(x.shape)\n",
    "\n",
    "net = MLP(num_inputs,num_hiddens,num_outputs)\n",
    "print(net)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467ea66",
   "metadata": {},
   "source": [
    "### Module的子类:\n",
    "    Module类是一个通用的部件\n",
    "    PyTorch还实现了继承自Module的可以方便构建模型的类: 如 Sequential、ModuleList 和 ModuleDict等等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a34e1",
   "metadata": {},
   "source": [
    "#### Sequential类:\n",
    "    当模型的前向计算为简单串联各个层的计算时，Sequential类可以通过更加简单的方式定义模型\n",
    "        Sequential类的目的：它可以接收一个子模块的有序字典（OrderedDict）或者一系列子模块作为参数来逐一添加Module的实例\n",
    "        \n",
    "    通过该方法定义，模型中的各层是有顺序的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6ac39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs,num_hiddens),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(num_hiddens,num_outputs)\n",
    ")\n",
    "# 等价于 net = Net() 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fd4bbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e044f0",
   "metadata": {},
   "source": [
    "#### ModuleList类:\n",
    "    ModuleList接收一个子模块的列表作为输入，然后可以类似List那样进行append和extend操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5eaa2ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append前：\n",
      " ModuleList(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "append后:\n",
      " ModuleList(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (3): Linear(in_features=256, out_features=128, bias=True)\n",
      ")\n",
      "通过索引访问各层:\n",
      " Linear(in_features=256, out_features=128, bias=True)\n"
     ]
    }
   ],
   "source": [
    "net = torch.nn.ModuleList([torch.nn.Linear(num_inputs,256),torch.nn.ReLU()])\n",
    "print('append前：\\n',net)\n",
    "net.append(torch.nn.Linear(128,num_outputs))\n",
    "net.append(torch.nn.Linear(256,128))\n",
    "print('append后:\\n',net)\n",
    "print('通过索引访问各层:\\n',net[-1]) # # 类似List的索引访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "336c14c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = net[0](x)\n",
    "out = net[1](out)\n",
    "out = net[3](out)\n",
    "out = net[1](out)\n",
    "result = net[2](out)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fcc90e",
   "metadata": {},
   "source": [
    "##### Sequential和ModuleList都可以进行列表化构造网络，区别:\n",
    "    ModuleList仅仅是一个储存各种模块的列表，模块之间没有联系也没有顺序,\n",
    "        即不用保证相邻层的输入输出维度匹配，而且没有实现forward功能需要自己实现\n",
    "        \n",
    "    Sequential内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配，内部forward功能已经实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b4dd88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleList的出现只是让网络定义前向传播时更加灵活\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule,self).__init__()\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(28*28,256),\n",
    "            torch.nn.Linear(256,10)\n",
    "        ])\n",
    "        self.act = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         x = x.view(-1,10)\n",
    "        for i,l in enumerate(self.linears):\n",
    "            x = self.linears[i](x)\n",
    "            x = self.act(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a6865366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModule(\n",
       "  (linears): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       "  (act): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MyModule()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e6200fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5111, 0.4616, 0.5501, 0.4119, 0.6059, 0.5918, 0.5332, 0.5335, 0.4305,\n",
       "         0.4957],\n",
       "        [0.5402, 0.4827, 0.5329, 0.4475, 0.6109, 0.5786, 0.4966, 0.5325, 0.4122,\n",
       "         0.5174]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,28*28)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9cc37",
   "metadata": {},
   "source": [
    "#### ModuleDict类:\n",
    "    ModuleDict接收一个子模块的字典作为输入, 然后也可以类似字典那样进行添加访问操作\n",
    "    \n",
    "    和ModuleList一样，ModuleDict实例仅仅是存放了一些模块的字典，并没有定义forward函数需要自己定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b73ff1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.ModuleDict({\n",
    "    'linear_1':torch.nn.Linear(28*28,256),\n",
    "    'act':torch.nn.ReLU(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "70db74bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "添加前:\n",
      " ModuleDict(\n",
      "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n",
      "添加后:\n",
      " ModuleDict(\n",
      "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "访问: ReLU()\n"
     ]
    }
   ],
   "source": [
    "print('添加前:\\n',net)\n",
    "net['output'] = torch.nn.Linear(256,10)\n",
    "print('添加后:\\n',net)\n",
    "print('访问:',net['act'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca18074",
   "metadata": {},
   "source": [
    "### 构造复杂的模型:\n",
    "    在下例网络中，创建一个不被迭代的参数，即常数参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ee88c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyMLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FancyMLP,self).__init__()\n",
    "        \n",
    "        self.rand_weight = torch.rand((20,20),requires_grad=True) # 不可训练参数\n",
    "        \n",
    "        self.linear = torch.nn.Linear(20,20)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        # 使用创建的常数参数，以及nn.functional中的relu函数和mm函数\n",
    "        x = torch.nn.functional.relu(torch.mm(x,self.rand_weight))\n",
    "        # 复用全连接层。等价于两个全连接层共享参数\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d32c12a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FancyMLP(\n",
      "  (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0990, -0.1480, -0.0983,  0.0971,  0.0212,  0.2084, -0.2221,  0.1330,\n",
       "          0.0380, -0.0148, -0.1950, -0.1751, -0.2196, -0.0632,  0.1342, -0.1370,\n",
       "         -0.0956, -0.0848,  0.0536, -0.1580],\n",
       "        [ 0.9119, -0.0256, -1.0380,  1.1922,  0.1304, -0.6293, -0.9572, -0.3028,\n",
       "         -0.7028,  0.2416, -0.1597, -0.4243, -1.3067,  0.5696, -1.2758,  0.2742,\n",
       "         -0.0214,  0.2981,  0.4599, -0.2188]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,20)\n",
    "net = FancyMLP()\n",
    "print(net)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9f10ff53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): FancyMLP(\n",
       "    (linear): Linear(in_features=20, out_features=20, bias=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=10, bias=True)\n",
       "  )\n",
       "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因为 FancyMLP 和 Sequential 类都是 Module 类的子类，所以可以嵌套调用它们\n",
    "net = torch.nn.Sequential(torch.nn.Linear(20,10))\n",
    "net = torch.nn.Sequential(FancyMLP(),net,torch.nn.Linear(10,1))\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1812a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
